.特征变换
（1）数值型-归一化（最大最小化）
通过对原始数据进行变换把数据映射到(new_max,new_min)(默认为[0,1])之间
缺点:最大值与最小值非常容易受异常点影响，这种方法鲁棒性较差，只适合传统精确小数据场景。
def min_max_scaler():
    from sklearn.preprocessing import MinMaxScaler
    #实例化MinMaxScaler()
    mm = MinMaxScaler()
    # 调用fit_transform()输入并转换数据
    data = mm.fit_transform([[90,2,10,40],
                             [60,4,15,45],
                             [75,3,13,46]])
    # 转换后的数据
    print(data)
min_max_scaler()

（2）数值型-标准化方法
通过对原始数据进行变换把数据变换到均值为0，方差为1范围内 如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。
def standars_scaler():
    from sklearn.preprocessing import StandardScaler
    #实例化StandardScaler()
    standard = StandardScaler()
    #调用fit_transform()输入并转换数据
    data = standard.fit_transform([[1,-1,3],
                                   [2,4,2],
                                   [4,6,1]])
    # 输出标准化后的数据矩阵
    print(data)
    # 输出每个特征的均值。
    print(standard.mean_)
    # 输出每个特征的标准差。
    print(standard.scale_)
standars_scaler()

（3）类别型-标签编码
def label_encoder():
    from sklearn.preprocessing import LabelEncoder
    import pandas as pd

    # 创建一个包含学生和他们爱好的数据框
    data = {
        '学生': ['小明', '小红', '小亮', '小张', '小李'],
        '爱好': ['游泳', '跑步', '游泳', '篮球', '跑步']
    }
    df = pd.DataFrame(data)
    # 创建 LabelEncoder 实例
    encoder = LabelEncoder()

    # 对 '爱好' 列进行编码
    df['爱好编码'] = encoder.fit_transform(df['爱好'])

    # 输出结果
    print("原始数据和编码后的数据:")
    print(df)

    # 查看编码对应的类别
    print("\n编码对应的类别:")
    for i, item in enumerate(encoder.classes_):
        print(f"{i}: {item}")

    # 将数值标签转换回原始类别标签
    decoded= encoder.inverse_transform(df['爱好编码'])
    print("解码后的名称:", decoded)

（4）类别型-独热编码
def one_hot_encoder():
    from sklearn.preprocessing import OneHotEncoder
    import pandas as pd

    # 创建一个包含学生和他们爱好的数据框
    data = {
        '学生': ['小明', '小红', '小亮', '小张', '小李'],
        '爱好': ['游泳', '跑步', '游泳', '篮球', '跑步']
    }
    df = pd.DataFrame(data)
    
    # 创建 OneHotEncoder 实例
    encoder = OneHotEncoder(sparse_output=False) # sparse=False 使得输出为普通数组，而不是稀疏矩阵
    # 对 '爱好' 列进行独热编码
    hobby_encoded = encoder.fit_transform(df[['爱好']])
    # 将编码后的数据转换为 DataFrame
    hobby_encoded_df = pd.DataFrame(hobby_encoded, columns=encoder.categories_[0])
    # 将原始数据与编码后的数据合并
    df_encoded = pd.concat([df, hobby_encoded_df], axis=1)

    # 输出结果
    print("原始数据和独热编码后的数据:")
    print(df_encoded)

    # 查看编码后的类别
    print("\n独热编码对应的类别:")
    print(encoder.categories_[0])                                               
 

2 特征选择
假设我们已经收集到如下的学生信息数据：
步骤 1：数据准备
首先，我们将数据载入并进行适当的处理。
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# 创建学生信息数据集
data = {
    '学号': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012', 'S013', 'S014', 'S015'],
    '姓名': ['张三', '李四', '王五', '赵六', '钱七', '孙八', '周九', '吴十', '郑十一', '冯十二', '陈十三', '蔡十四', '林十五', '张十六', '刘十七'],
    '性别': ['男', '女', '男', '女', '男', '女', '男', '女', '男', '女', '男', '女', '男', '女', '男'],
    '年龄': [18, 19, 17, 20, 18, 19, 17, 21, 18, 19, 20, 18, 17, 19, 18],
    '数学成绩': [85, 76, 90, 60, 78, 85, 55, 95, 70, 88, 65, 82, 90, 75, 89],
    '语文成绩': [78, 82, 75, 55, 80, 87, 60, 92, 72, 90, 60, 79, 85, 70, 80],
    '英语成绩': [70, 75, 80, 50, 60, 65, 60, 90, 60, 85, 65, 70, 95, 55, 80],
    '体育成绩': [92, 80, 88, 65, 85, 90, 58, 88, 75, 91, 68, 85, 93, 72, 87],
    '家庭年收入': [60000, 55000, 45000, 70000, 55000, 60000, 40000, 80000, 50000, 75000, 46000, 62000, 68000, 53000, 65000],
    '参加课外活动': [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1]
}

# 将数据转换为DataFrame
df = pd.DataFrame(data)
df.head()
 

步骤 2：特征变换-准备特征和目标变量
# 进行数据预处理，处理性别列
le = LabelEncoder()
df['性别'] = le.fit_transform(df['性别'])

# 特征和标签
X = df.drop(['学号', '姓名', '参加课外活动'], axis=1)
y = df['参加课外活动']

# 拆分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
步骤 3：特征选择方法
（1）. 过滤式特征选择（Filter Method）
使用皮尔逊相关系数来选择特征。
# 计算特征与目标变量的相关系数
correlation_matrix = X.corrwith(y)

# 输出相关系数
print("特征与目标变量的相关系数：")
print(correlation_matrix)

# 根据相关系数选择重要特征（设定阈值）
selected_features_filter = correlation_matrix[correlation_matrix > 0.8]
print(f"选择的特征（过滤式方法） ：\n {selected_features_filter}")

（2）嵌入式特征选择（Embedded Method）
使用随机森林作为嵌入式方法来选择特征
from sklearn.ensemble import RandomForestClassifier

# 创建并训练随机森林分类器
rf = RandomForestClassifier( random_state=42)
rf.fit(X_train, y_train)
# 获取特征的重要性
feature_importance = rf.feature_importances_

# 显示特征重要性
features = X.columns
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importance
}).sort_values(by='Importance', ascending=False)

print("特征重要性（嵌入式方法，随机森林）:")
print(importance_df)
 

（3）包裹式特征选择（Wrapper Method）
使用**递归特征消除（RFE）**来选择特征。这里我们使用岭回归模型来进行特征消除。
# 使用递归特征消除（RFE）进行特征选择
# 创建岭回归模型
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1.0)
rfe = RFE(estimator=ridge, n_features_to_select=3)
rfe.fit(X_train, y_train)
# 查看被选择的特征排名
print(rfe.ranking_)
# 获取选中的特征
selected_features_rfe = X.columns[rfe.support_]
print(f"选择的特征（包裹式方法，RFE）：{selected_features_rfe}")
 